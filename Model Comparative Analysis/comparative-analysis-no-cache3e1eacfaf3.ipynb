{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T14:15:09.423709Z",
     "iopub.status.busy": "2025-12-04T14:15:09.423461Z",
     "iopub.status.idle": "2025-12-04T14:15:26.033259Z",
     "shell.execute_reply": "2025-12-04T14:15:26.032591Z",
     "shell.execute_reply.started": "2025-12-04T14:15:09.423691Z"
    },
    "id": "kJLG7VcnmRTt",
    "outputId": "b3519b06-5eee-4c0e-943c-e65e192ddf83",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 14:15:10.855694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764857711.037638      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764857711.093040      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, VGG19, InceptionV3, Xception, ResNet50, DenseNet121\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T14:15:32.407603Z",
     "iopub.status.busy": "2025-12-04T14:15:32.406626Z",
     "iopub.status.idle": "2025-12-04T14:15:32.708736Z",
     "shell.execute_reply": "2025-12-04T14:15:32.708070Z",
     "shell.execute_reply.started": "2025-12-04T14:15:32.407574Z"
    },
    "id": "RCmICxH4mjgx",
    "outputId": "48c4d947-97a2-4ede-e502-ba009307eac0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/new-plant-diseases-dataset\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"vipoooool/new-plant-diseases-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:15:36.531569Z",
     "iopub.status.busy": "2025-12-04T14:15:36.531280Z",
     "iopub.status.idle": "2025-12-04T14:15:36.535463Z",
     "shell.execute_reply": "2025-12-04T14:15:36.534768Z",
     "shell.execute_reply.started": "2025-12-04T14:15:36.531547Z"
    },
    "id": "Y-DmJZw5mRTu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "# For Kaggle kernel:\n",
    "train = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'\n",
    "valid = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'\n",
    "\n",
    "# train = \"C:\\\\Users\\\\junu\\\\.cache\\\\kagglehub\\\\datasets\\\\vipoooool\\\\new-plant-diseases-dataset\\\\versions\\\\2\\\\New Plant Diseases Dataset(Augmented)\\\\New Plant Diseases Dataset(Augmented)\\\\train\"\n",
    "# valid = \"C:\\\\Users\\\\junu\\\\.cache\\\\kagglehub\\\\datasets\\\\vipoooool\\\\new-plant-diseases-dataset\\\\versions\\\\2\\\\New Plant Diseases Dataset(Augmented)\\\\New Plant Diseases Dataset(Augmented)\\\\valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T14:15:39.885041Z",
     "iopub.status.busy": "2025-12-04T14:15:39.884569Z",
     "iopub.status.idle": "2025-12-04T14:15:39.889231Z",
     "shell.execute_reply": "2025-12-04T14:15:39.888535Z",
     "shell.execute_reply.started": "2025-12-04T14:15:39.885017Z"
    },
    "id": "nKbJD82tmRTv",
    "outputId": "93415f07-8c8f-4e44-e8a7-75046ed4b7ec",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (299, 299)\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "image_size = (256, 256)  \n",
    "batch_size = 32          \n",
    "\n",
    "print(f\"Image size: {image_size}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T14:15:43.600414Z",
     "iopub.status.busy": "2025-12-04T14:15:43.600136Z",
     "iopub.status.idle": "2025-12-04T14:16:06.719915Z",
     "shell.execute_reply": "2025-12-04T14:16:06.719285Z",
     "shell.execute_reply.started": "2025-12-04T14:15:43.600393Z"
    },
    "id": "sy3p_eWQmRTv",
    "outputId": "b5e04546-0c83-4edc-a0ad-c16be3736d24",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Found 70295 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764857762.504696      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17572 files belonging to 38 classes.\n",
      "Number of classes: 38\n",
      "\n",
      "✓ Datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create datasets WITHOUT caching\n",
    "def create_datasets():\n",
    "    \"\"\"\n",
    "    Create training and validation datasets WITHOUT caching.\n",
    "    Data will be loaded fresh from disk each epoch.\n",
    "    Uses minimal memory but slower training.\n",
    "    \"\"\"\n",
    "    # Training dataset\n",
    "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        train,\n",
    "        seed=123,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Validation dataset\n",
    "    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        valid,\n",
    "        seed=123,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Save class names\n",
    "    class_names = train_dataset.class_names\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    total_batches = train_dataset.cardinality().numpy()\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(\n",
    "        buffer_size=100,\n",
    "        reshuffle_each_iteration=True\n",
    "    )\n",
    "\n",
    "    portion = 0.01\n",
    "    train_dataset = train_dataset.take(int(total_batches * portion))\n",
    "\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomFlip(\"vertical\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.2),\n",
    "    ])\n",
    "\n",
    "    normalization = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda x, y: (data_augmentation(x, training=True), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda x, y: (normalization(x), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset, class_names, num_classes\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset, val_dataset, class_names, num_classes = create_datasets()\n",
    "print(\"\\n✓ Datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T14:16:30.104590Z",
     "iopub.status.busy": "2025-12-04T14:16:30.104286Z",
     "iopub.status.idle": "2025-12-04T14:16:30.118171Z",
     "shell.execute_reply": "2025-12-04T14:16:30.117253Z",
     "shell.execute_reply.started": "2025-12-04T14:16:30.104568Z"
    },
    "id": "EpKKWDDOmRTw",
    "outputId": "1c38b191-7f6c-45be-9210-c97102b02f0f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_model(base_model_class, model_name, num_classes):\n",
    "\n",
    "    base_model = base_model_class(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=image_size + (3,)\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        jit_compile=True\n",
    "    )\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "def fine_tune_model(model, base_model, num_layers_to_unfreeze=20):\n",
    "    \"\"\"\n",
    "    Fine-tune the model by unfreezing top layers.\n",
    "    \"\"\"\n",
    "    base_model.trainable = True\n",
    "\n",
    "    for layer in base_model.layers[:-num_layers_to_unfreeze]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_callbacks(model_name):\n",
    "    \"\"\"\n",
    "    Create callbacks for training.\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'{model_name}_best.weights.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "def train_single_model(model_name, base_model_class):\n",
    "    \"\"\"\n",
    "    Train a single model with two-stage training.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\nCreating {model_name} model...\")\n",
    "    model, base_model = create_model(base_model_class, model_name, num_classes)\n",
    "    print(f\"✓ {model_name} model created\")\n",
    "\n",
    "    print(f\"\\n[STAGE 1] Initial training with frozen base model...\")\n",
    "    initial_epochs = 10\n",
    "\n",
    "    history_stage1 = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=initial_epochs,\n",
    "        callbacks=get_callbacks(f\"{model_name}_stage1\")\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[STAGE 2] Fine-tuning with unfrozen top layers...\")\n",
    "    model = fine_tune_model(model, base_model, num_layers_to_unfreeze=20)\n",
    "    fine_tune_epochs = 10\n",
    "\n",
    "    history_stage2 = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=fine_tune_epochs,\n",
    "        callbacks=get_callbacks(f\"{model_name}_stage2\")\n",
    "    )\n",
    "\n",
    "    model.save(f\"{model_name}_plant_disease_model.h5\")\n",
    "    print(f\"\\n✓ {model_name} model saved\")\n",
    "\n",
    "    print(f\"\\nGenerating predictions for {model_name}...\")\n",
    "    y_pred = model.predict(val_dataset)\n",
    "\n",
    "    y_true = []\n",
    "    for image_batch, label_batch in val_dataset:\n",
    "        y_true.append(label_batch)\n",
    "    y_true = tf.concat(y_true, axis=0).numpy()\n",
    "\n",
    "    np.save(f\"{model_name}_y_pred.npy\", y_pred)\n",
    "    np.save(f\"{model_name}_y_true.npy\", y_true)\n",
    "    print(f\"✓ {model_name} predictions saved\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    y_pred_labels = y_pred.argmax(axis=1)\n",
    "    y_true_labels = y_true.argmax(axis=1)\n",
    "    accuracy = np.mean(y_pred_labels == y_true_labels)\n",
    "\n",
    "    print(f\"\\n{model_name} FINAL VALIDATION ACCURACY: {accuracy*100:.2f}%\")\n",
    "\n",
    "    if accuracy >= 0.985:\n",
    "        print(f\"✓ {model_name} achieved >=98.5% accuracy!\")\n",
    "    else:\n",
    "        print(f\"✗ {model_name} did not reach 98.5% accuracy (got {accuracy*100:.2f}%)\")\n",
    "\n",
    "    # Clear memory before next model\n",
    "    del model, base_model\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XAHNx9OmRTw"
   },
   "source": [
    "---\n",
    "## Model Training Cells\n",
    "**Run each cell below to train that specific model. Skip any cell to exclude that model from training.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXcKwddjmRTx"
   },
   "outputs": [],
   "source": [
    "# Train VGG16\n",
    "try:\n",
    "    vgg16_accuracy = train_single_model('VGG16', VGG16)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training VGG16: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJQDASq7mRTy"
   },
   "outputs": [],
   "source": [
    "# Train VGG19\n",
    "try:\n",
    "    vgg19_accuracy = train_single_model('VGG19', VGG19)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training VGG19: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aN6aaNBumRTz"
   },
   "outputs": [],
   "source": [
    "# Train InceptionV3\n",
    "try:\n",
    "    inceptionv3_accuracy = train_single_model('InceptionV3', InceptionV3)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training InceptionV3: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3p8_BeimRTz"
   },
   "outputs": [],
   "source": [
    "# Train Xception\n",
    "try:\n",
    "    xception_accuracy = train_single_model('Xception', Xception)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training Xception: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:16:38.951844Z",
     "iopub.status.busy": "2025-12-04T14:16:38.951537Z",
     "iopub.status.idle": "2025-12-04T14:31:37.371119Z",
     "shell.execute_reply": "2025-12-04T14:31:37.370398Z",
     "shell.execute_reply.started": "2025-12-04T14:16:38.951821Z"
    },
    "id": "_m6xoV2-mRTz",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING ResNet50\n",
      "================================================================================\n",
      "\n",
      "Creating ResNet50 model...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "✓ ResNet50 model created\n",
      "\n",
      "[STAGE 1] Initial training with frozen base model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764857826.969318     115 service.cc:148] XLA service 0x7d81580029d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764857826.970180     115 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1764857829.598358     115 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:49\u001b[0m 39s/step - accuracy: 0.0312 - loss: 4.2709"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764857840.148884     115 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - accuracy: 0.0281 - loss: 4.3379\n",
      "Epoch 1: val_accuracy improved from -inf to 0.02760, saving model to ResNet50_stage1_best.weights.h5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 6s/step - accuracy: 0.0280 - loss: 4.3392 - val_accuracy: 0.0276 - val_loss: 3.7424 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655ms/step - accuracy: 0.0277 - loss: 4.1151\n",
      "Epoch 2: val_accuracy did not improve from 0.02760\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0284 - loss: 4.1147 - val_accuracy: 0.0276 - val_loss: 3.8241 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644ms/step - accuracy: 0.0705 - loss: 3.8106\n",
      "Epoch 3: val_accuracy improved from 0.02760 to 0.02766, saving model to ResNet50_stage1_best.weights.h5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0705 - loss: 3.8094 - val_accuracy: 0.0277 - val_loss: 3.9949 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641ms/step - accuracy: 0.0629 - loss: 3.7391\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.02766\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0636 - loss: 3.7372 - val_accuracy: 0.0274 - val_loss: 4.0102 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643ms/step - accuracy: 0.0596 - loss: 3.6478\n",
      "Epoch 5: val_accuracy did not improve from 0.02766\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0614 - loss: 3.6420 - val_accuracy: 0.0274 - val_loss: 4.0354 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641ms/step - accuracy: 0.1047 - loss: 3.5380\n",
      "Epoch 6: val_accuracy did not improve from 0.02766\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.1052 - loss: 3.5403 - val_accuracy: 0.0274 - val_loss: 3.9830 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643ms/step - accuracy: 0.0958 - loss: 3.5611\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.02766\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0978 - loss: 3.5502 - val_accuracy: 0.0273 - val_loss: 3.9407 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646ms/step - accuracy: 0.1189 - loss: 3.4679\n",
      "Epoch 8: val_accuracy did not improve from 0.02766\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.1190 - loss: 3.4641 - val_accuracy: 0.0274 - val_loss: 3.9166 - learning_rate: 2.5000e-04\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "[STAGE 2] Fine-tuning with unfrozen top layers...\n",
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.0589 - loss: 3.9221\n",
      "Epoch 1: val_accuracy improved from -inf to 0.04058, saving model to ResNet50_stage2_best.weights.h5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - accuracy: 0.0591 - loss: 3.9190 - val_accuracy: 0.0406 - val_loss: 3.9752 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663ms/step - accuracy: 0.0553 - loss: 3.9244\n",
      "Epoch 2: val_accuracy did not improve from 0.04058\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0562 - loss: 3.9117 - val_accuracy: 0.0252 - val_loss: 3.9642 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651ms/step - accuracy: 0.0602 - loss: 3.8245\n",
      "Epoch 3: val_accuracy did not improve from 0.04058\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0600 - loss: 3.8295 - val_accuracy: 0.0265 - val_loss: 3.9432 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645ms/step - accuracy: 0.0820 - loss: 3.7469\n",
      "Epoch 4: val_accuracy did not improve from 0.04058\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0808 - loss: 3.7474 - val_accuracy: 0.0241 - val_loss: 3.8979 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653ms/step - accuracy: 0.0733 - loss: 3.6682\n",
      "Epoch 5: val_accuracy did not improve from 0.04058\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0736 - loss: 3.6685 - val_accuracy: 0.0256 - val_loss: 3.8395 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634ms/step - accuracy: 0.0837 - loss: 3.6558\n",
      "Epoch 6: val_accuracy did not improve from 0.04058\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 5s/step - accuracy: 0.0833 - loss: 3.6582 - val_accuracy: 0.0260 - val_loss: 3.7786 - learning_rate: 1.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ ResNet50 model saved\n",
      "\n",
      "Generating predictions for ResNet50...\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 162ms/step\n",
      "✓ ResNet50 predictions saved\n",
      "\n",
      "ResNet50 FINAL VALIDATION ACCURACY: 4.06%\n",
      "✗ ResNet50 did not reach 98.5% accuracy (got 4.06%)\n"
     ]
    }
   ],
   "source": [
    "# Train ResNet50\n",
    "try:\n",
    "    resnet50_accuracy = train_single_model('ResNet50', ResNet50)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training ResNet50: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "execution_failed": "2025-12-04T13:56:01.121Z",
     "iopub.execute_input": "2025-12-04T13:54:59.183827Z",
     "iopub.status.busy": "2025-12-04T13:54:59.183065Z"
    },
    "id": "tk4KdR7wmRT0",
    "outputId": "d69c7713-f01e-4844-d98d-c33192a68297",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING DenseNet121\n",
      "================================================================================\n",
      "\n",
      "Creating DenseNet121 model...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "✓ DenseNet121 model created\n",
      "\n",
      "[STAGE 1] Initial training with frozen base model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 13:55:49.369125: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1253] failed to alloc 17179869184 bytes on host: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "W0000 00:00:1764856549.369186     186 device_host_allocator.h:61] could not allocate pinned host memory of size: 17179869184\n"
     ]
    }
   ],
   "source": [
    "# Train DenseNet121\n",
    "try:\n",
    "    densenet121_accuracy = train_single_model('DenseNet121', DenseNet121)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training DenseNet121: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfY9K7i0mRT0"
   },
   "source": [
    "---\n",
    "## Results & Comparison\n",
    "**Run the cells below after training your models to see the comparison.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mv1RLDl2mRT0"
   },
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List of all possible models\n",
    "all_models = ['VGG16', 'VGG19', 'InceptionV3', 'Xception', 'ResNet50', 'DenseNet121']\n",
    "results = []\n",
    "\n",
    "for model_name in all_models:\n",
    "    try:\n",
    "        # Load predictions\n",
    "        y_pred = np.load(f\"{model_name}_y_pred.npy\")\n",
    "        y_true = np.load(f\"{model_name}_y_true.npy\")\n",
    "\n",
    "        # Convert to class labels\n",
    "        y_pred_labels = y_pred.argmax(axis=1)\n",
    "        y_true_labels = y_true.argmax(axis=1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = np.mean(y_pred_labels == y_true_labels)\n",
    "        precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "        recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "        f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "\n",
    "        # Top-5 accuracy\n",
    "        top5_pred = np.argsort(y_pred, axis=1)[:, -5:]\n",
    "        top5_accuracy = np.mean([y_true_labels[i] in top5_pred[i] for i in range(len(y_true_labels))])\n",
    "\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy (%)': accuracy * 100,\n",
    "            'Precision (%)': precision * 100,\n",
    "            'Recall (%)': recall * 100,\n",
    "            'F1-Score (%)': f1 * 100,\n",
    "            'Top-5 Accuracy (%)': top5_accuracy * 100,\n",
    "            'Meets Target (>98.5%)': '✓' if accuracy > 0.985 else '✗'\n",
    "        })\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️  {model_name} was not trained (skipped)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load results for {model_name}: {str(e)}\")\n",
    "\n",
    "if len(results) > 0:\n",
    "    # Create DataFrame\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    comparison_df = comparison_df.sort_values('Accuracy (%)', ascending=False)\n",
    "\n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    # Count models above target\n",
    "    models_above_target = comparison_df[comparison_df['Meets Target (>98.5%)'] == '✓'].shape[0]\n",
    "    print(f\"\\nModels achieving >98.5% accuracy: {models_above_target}/{len(results)}\")\n",
    "\n",
    "    # Save comparison table\n",
    "    comparison_df.to_csv('model_comparison_sequential.csv', index=False)\n",
    "    print(\"\\n✓ Comparison table saved to 'model_comparison_sequential.csv'\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No models were trained. Please run at least one model training cell above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQPFtulemRT0"
   },
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if len(results) > 0:\n",
    "    # Set style\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot accuracy comparison\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = ['green' if x == '✓' else 'red' for x in comparison_df['Meets Target (>98.5%)']]\n",
    "    plt.barh(comparison_df['Model'], comparison_df['Accuracy (%)'], color=colors, alpha=0.7)\n",
    "    plt.axvline(x=98.5, color='blue', linestyle='--', label='Target (98.5%)')\n",
    "    plt.xlabel('Accuracy (%)')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Plot metrics comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    metrics_df = comparison_df[['Model', 'Precision (%)', 'Recall (%)', 'F1-Score (%)']].set_index('Model')\n",
    "    metrics_df.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "    plt.ylabel('Score (%)')\n",
    "    plt.title('Model Metrics Comparison')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('model_comparison_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"✓ Visualization saved to 'model_comparison_visualization.png'\")\n",
    "else:\n",
    "    print(\"⚠️  No results to visualize. Train at least one model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m72gSaybmRT1"
   },
   "outputs": [],
   "source": [
    "# Generate detailed classification reports\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in all_models:\n",
    "    try:\n",
    "        y_pred = np.load(f\"{model_name}_y_pred.npy\")\n",
    "        y_true = np.load(f\"{model_name}_y_true.npy\")\n",
    "\n",
    "        y_pred_labels = y_pred.argmax(axis=1)\n",
    "        y_true_labels = y_true.argmax(axis=1)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{model_name} - Classification Report\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        report = classification_report(y_true_labels, y_pred_labels, target_names=class_names)\n",
    "        print(report)\n",
    "\n",
    "        # Save report\n",
    "        with open(f\"{model_name}_classification_report.txt\", 'w') as f:\n",
    "            f.write(f\"{model_name} - Classification Report\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(report)\n",
    "\n",
    "        print(f\"✓ Report saved to '{model_name}_classification_report.txt'\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n⚠️  {model_name} was not trained (skipped)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not generate report for {model_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qM4UZ7PNmRT1"
   },
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(results) > 0:\n",
    "    print(f\"\\nTotal models trained: {len(results)}\")\n",
    "    print(f\"Models achieving >98.5% accuracy: {models_above_target}\")\n",
    "    print(f\"\\nBest performing model: {comparison_df.iloc[0]['Model']}\")\n",
    "    print(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy (%)']:.2f}%\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No models were trained.\")\n",
    "\n",
    "print(\"\\nKey features implemented:\")\n",
    "print(\"  ✓ Sequential training (one model at a time)\")\n",
    "print(\"  ✓ Separate cells for each model (skip any model)\")\n",
    "print(\"  ✓ NO caching (minimal memory usage)\")\n",
    "print(\"  ✓ Memory management between models\")\n",
    "print(\"  ✓ GPU memory growth enabled\")\n",
    "print(\"  ✓ Mixed precision training\")\n",
    "print(\"  ✓ Two-stage training with fine-tuning\")\n",
    "print(\"  ✓ Enhanced data augmentation\")\n",
    "print(\"  ✓ Learning rate scheduling\")\n",
    "print(\"  ✓ Early stopping and model checkpointing\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 193494,
     "datasetId": 78313,
     "isSourceIdPinned": false,
     "sourceId": 182633,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
