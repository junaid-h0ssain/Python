{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31fc48",
   "metadata": {},
   "source": [
    "# Train 6 CNN models with mixed precision\n",
    "This notebook prepares a plant-disease dataset, enables mixed precision, and trains six models (VGG16, VGG19, InceptionV3, Xception, ResNet50, DenseNet121) using `image_dataset_from_directory`.\n",
    "Notes: run the first code cell to install dependencies if needed, then run cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b2cf500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'new-plant-diseases-dataset' dataset.\n",
      "Path to dataset files: /kaggle/input/new-plant-diseases-dataset\n",
      "Path to dataset files: /kaggle/input/new-plant-diseases-dataset\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run if packages missing)\n",
    "# Uncomment to run installs inside the notebook environment\n",
    "# !pip install -q tensorflow tensorflow-io kagglehub matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25511241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision policy set to: <DTypePolicy \"mixed_float16\">\n"
     ]
    }
   ],
   "source": [
    "# Imports and mixed precision setup\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "# Enable mixed precision for faster training on modern GPUs\n",
    "try:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print('Mixed precision policy set to:', tf.keras.mixed_precision.global_policy())\n",
    "except Exception as e:\n",
    "    print('Could not set mixed precision policy:', e)\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cae9f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download dataset with kagglehub...\n",
      "Using Colab cache for faster access to the 'new-plant-diseases-dataset' dataset.\n",
      "Using dataset path: /kaggle/input/new-plant-diseases-dataset\n",
      "Using Colab cache for faster access to the 'new-plant-diseases-dataset' dataset.\n",
      "Using dataset path: /kaggle/input/new-plant-diseases-dataset\n"
     ]
    }
   ],
   "source": [
    "# Dataset path: try to download with kagglehub if available, else set `data_dir` manually.\n",
    "import zipfile\n",
    "data_dir = None\n",
    "try:\n",
    "    import kagglehub\n",
    "    print('Attempting to download dataset with kagglehub...')\n",
    "    path = kagglehub.dataset_download('vipoooool/new-plant-diseases-dataset')\n",
    "    # kagglehub.dataset_download often returns a zip path; try to extract\n",
    "    if path:\n",
    "        if path.endswith('.zip') and os.path.exists(path):\n",
    "            extract_to = os.path.splitext(path)[0] + '_extracted'\n",
    "            with zipfile.ZipFile(path, 'r') as zf:\n",
    "                zf.extractall(extract_to)\n",
    "            data_dir = extract_to\n",
    "            print('Extracted dataset to', data_dir)\n",
    "        else:\n",
    "            # If path is a directory or already extracted\n",
    "            data_dir = path\n",
    "            print('Using dataset path:', data_dir)\n",
    "except Exception as e:\n",
    "    print('kagglehub not available or download failed:', e)\n",
    "    # Fallback: user should edit this path to point to local dataset directory\n",
    "if data_dir is None:\n",
    "    # EDIT this if your dataset is already available locally\n",
    "    data_dir = '/path/to/plant-disease-dataset'  # <-- change this to your dataset folder\n",
    "    print('Please set `data_dir` to your dataset directory. Current value:', data_dir)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10b8bc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 175767 files belonging to 3 classes.\n",
      "Using 140614 files for training.\n",
      "Using 140614 files for training.\n",
      "Found 175767 files belonging to 3 classes.\n",
      "Found 175767 files belonging to 3 classes.\n",
      "Using 35153 files for validation.\n",
      "Using 35153 files for validation.\n",
      "Classes: ['New Plant Diseases Dataset(Augmented)', 'new plant diseases dataset(augmented)', 'test']\n",
      "Number of classes: 3\n",
      "Classes: ['New Plant Diseases Dataset(Augmented)', 'new plant diseases dataset(augmented)', 'test']\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation/test datasets using image_dataset_from_directory\n",
    "# Adjust these parameters as needed\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "validation_split = 0.2\n",
    "seed = 123\n",
    "label_mode = 'categorical'\n",
    "# If dataset path doesn't exist, this cell will error - set `data_dir` first\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(f'Data directory not found: {data_dir} - please set the correct path')\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(str(data_dir),\n",
    "    validation_split=validation_split, subset='training', seed=seed,\n",
    "    image_size=image_size, batch_size=batch_size, label_mode=label_mode)\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(str(data_dir),\n",
    "    validation_split=validation_split, subset='validation', seed=seed,\n",
    "    image_size=image_size, batch_size=batch_size, label_mode=label_mode)\n",
    "# Optionally create a test set by splitting differently or using a separate folder\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print('Classes:', class_names)\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88acfd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance: cache and prefetch\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48d13971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Training VGG16 ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"VGG16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"VGG16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m1,539\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,716,227</span> (56.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,716,227\u001b[0m (56.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> (6.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,539\u001b[0m (6.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m 432/4395\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:17\u001b[0m 156ms/step - accuracy: 0.4410 - loss: 3.2106"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Model factory and training loop\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "models_to_train = {\n",
    "    'VGG16': applications.VGG16,\n",
    "    'VGG19': applications.VGG19,\n",
    "    'InceptionV3': applications.InceptionV3,\n",
    "    'Xception': applications.Xception,\n",
    "    'ResNet50': applications.ResNet50,\n",
    "    'DenseNet121': applications.DenseNet121,\n",
    "}\n",
    "# Training hyperparameters\n",
    "base_learning_rate = 1e-4\n",
    "head_epochs = 3\n",
    "fine_tune_epochs = 2\n",
    "models_dir = pathlib.Path('models')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "histories = {}\n",
    "for name, constructor in models_to_train.items():\n",
    "    print('== Training', name, '==')\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Build base model\n",
    "    try:\n",
    "        base = constructor(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
    "    except Exception as e:\n",
    "        print(f'Failed to construct {name} with imagenet weights: {e} - trying without weights')\n",
    "        base = constructor(weights=None, include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
    "    base.trainable = False\n",
    "    inputs = Input(shape=(image_size[0], image_size[1], 3))\n",
    "    x = base(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # Ensure final dense is float32 to avoid numeric issues with mixed precision\n",
    "    outputs = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    model = Model(inputs, outputs, name=name)\n",
    "    # Optimizer with loss scaling for mixed precision\n",
    "    base_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "    try:\n",
    "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer)\n",
    "    except Exception:\n",
    "        optimizer = base_optimizer\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Callbacks\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    ckpt = ModelCheckpoint(models_dir / f'{name}_best.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "    early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "    # Train head\n",
    "    history_head = model.fit(train_ds, validation_data=val_ds, epochs=head_epochs, callbacks=[ckpt, reduce_lr, early])\n",
    "    # Optionally fine-tune: unfreeze last block and continue training\n",
    "    base.trainable = True\n",
    "    # Recompile with lower LR\n",
    "    try:\n",
    "        base_optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10)\n",
    "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer)\n",
    "    except Exception:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history_fine = model.fit(train_ds, validation_data=val_ds, epochs=head_epochs + fine_tune_epochs, initial_epoch=history_head.epoch[-1]+1 if hasattr(history_head, 'epoch') and len(history_head.epoch)>0 else 0, callbacks=[ckpt, reduce_lr, early])\n",
    "    # Save final model\n",
    "    model.save(models_dir / f'{name}_final')\n",
    "    # Combine histories for plotting later\n",
    "    h = {}\n",
    "    for k,v in history_head.history.items():\n",
    "        h[k] = v.copy()\n",
    "    for k,v in history_fine.history.items():\n",
    "        # append fine tuning metrics\n",
    "        if k in h:\n",
    "            h[k].extend(v)\n",
    "        else:\n",
    "            h[k] = v.copy()\n",
    "    histories[name] = h\n",
    "    print(f'Finished training {name}. Saved to {models_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for all models\n",
    "import seaborn as sns\n",
    "for name, h in histories.items():\n",
    "    epochs = range(1, len(h['loss'])+1)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, h['loss'], label='train_loss')\n",
    "    plt.plot(epochs, h['val_loss'], label='val_loss')\n",
    "    plt.title(f'{name} Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, h['accuracy'], label='train_acc')\n",
    "    plt.plot(epochs, h['val_accuracy'], label='val_acc')\n",
    "    plt.title(f'{name} Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
