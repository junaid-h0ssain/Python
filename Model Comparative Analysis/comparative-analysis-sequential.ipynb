{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sequential Model Training with Memory Optimization\n",
                "This notebook trains models one by one with caching for efficient memory usage on Kaggle P100 GPU.\n",
                "\n",
                "**Instructions:** Run cells sequentially. Skip any model cell if you don't want to train that specific model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -U 'tensorflow[and-cuda]'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Required Libraries\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.applications import VGG16, VGG19, InceptionV3, Xception, ResNet50, DenseNet121\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import gc\n",
                "from tensorflow.keras import backend as K\n",
                "\n",
                "# Enable mixed precision for faster training\n",
                "from tensorflow.keras import mixed_precision\n",
                "mixed_precision.set_global_policy('mixed_float16')\n",
                "\n",
                "# Set memory growth to avoid OOM errors\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    try:\n",
                "        for gpu in gpus:\n",
                "            tf.config.experimental.set_memory_growth(gpu, True)\n",
                "        print(f\"GPU Available: {len(gpus)} GPU(s)\")\n",
                "    except RuntimeError as e:\n",
                "        print(e)\n",
                "else:\n",
                "    print(\"No GPU found\")\n",
                "\n",
                "print(\"TensorFlow version:\", tf.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download dataset (for Kaggle, use the input path directly)\n",
                "# Uncomment the following for local development:\n",
                "# import kagglehub\n",
                "# path = kagglehub.dataset_download(\"vipoooool/new-plant-diseases-dataset\")\n",
                "# print(\"Path to dataset files:\", path)\n",
                "\n",
                "# For Kaggle kernel:\n",
                "train = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'\n",
                "valid = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'\n",
                "\n",
                "# For local development:\n",
                "# train = \"C:\\\\Users\\\\junu\\\\.cache\\\\kagglehub\\\\datasets\\\\vipoooool\\\\new-plant-diseases-dataset\\\\versions\\\\2\\\\New Plant Diseases Dataset(Augmented)\\\\New Plant Diseases Dataset(Augmented)\\\\train\"\n",
                "# valid = \"C:\\\\Users\\\\junu\\\\.cache\\\\kagglehub\\\\datasets\\\\vipoooool\\\\new-plant-diseases-dataset\\\\versions\\\\2\\\\New Plant Diseases Dataset(Augmented)\\\\New Plant Diseases Dataset(Augmented)\\\\valid\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "image_size = (128, 128)  # Change to (224, 224) or (299, 299) for better accuracy\n",
                "batch_size = 64          # Reduce to 32 if using larger image sizes\n",
                "CACHE_DIR = '/kaggle/working/cache'  # Cache directory for preprocessed data\n",
                "os.makedirs(CACHE_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Image size: {image_size}\")\n",
                "print(f\"Batch size: {batch_size}\")\n",
                "print(f\"Cache directory: {CACHE_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create datasets with caching for memory efficiency\n",
                "def create_cached_datasets():\n",
                "    \"\"\"\n",
                "    Create training and validation datasets with caching.\n",
                "    This prevents reloading data from disk for each model.\n",
                "    \"\"\"\n",
                "    # Training dataset\n",
                "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
                "        train,\n",
                "        seed=123,\n",
                "        image_size=image_size,\n",
                "        batch_size=batch_size,\n",
                "        label_mode='categorical',\n",
                "        shuffle=True\n",
                "    )\n",
                "    \n",
                "    # Validation dataset\n",
                "    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
                "        valid,\n",
                "        seed=123,\n",
                "        image_size=image_size,\n",
                "        batch_size=batch_size,\n",
                "        label_mode='categorical',\n",
                "        shuffle=False\n",
                "    )\n",
                "    \n",
                "    # Save class names\n",
                "    class_names = train_dataset.class_names\n",
                "    num_classes = len(class_names)\n",
                "    \n",
                "    print(f\"Number of classes: {num_classes}\")\n",
                "    \n",
                "    # Data augmentation for training\n",
                "    data_augmentation = tf.keras.Sequential([\n",
                "        tf.keras.layers.Rescaling(1./255),\n",
                "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
                "        tf.keras.layers.RandomFlip(\"vertical\"),\n",
                "        tf.keras.layers.RandomRotation(0.2),\n",
                "        tf.keras.layers.RandomZoom(0.2),\n",
                "        tf.keras.layers.RandomContrast(0.2),\n",
                "    ])\n",
                "    \n",
                "    # Normalization for validation\n",
                "    normalization = tf.keras.layers.Rescaling(1./255)\n",
                "    \n",
                "    # Apply transformations and cache\n",
                "    train_dataset = train_dataset.map(\n",
                "        lambda x, y: (data_augmentation(x, training=True), y),\n",
                "        num_parallel_calls=tf.data.AUTOTUNE\n",
                "    ).cache(os.path.join(CACHE_DIR, 'train_cache')).prefetch(tf.data.AUTOTUNE)\n",
                "    \n",
                "    val_dataset = val_dataset.map(\n",
                "        lambda x, y: (normalization(x), y),\n",
                "        num_parallel_calls=tf.data.AUTOTUNE\n",
                "    ).cache(os.path.join(CACHE_DIR, 'val_cache')).prefetch(tf.data.AUTOTUNE)\n",
                "    \n",
                "    return train_dataset, val_dataset, class_names, num_classes\n",
                "\n",
                "# Create datasets once\n",
                "print(\"Creating and caching datasets...\")\n",
                "train_dataset, val_dataset, class_names, num_classes = create_cached_datasets()\n",
                "print(\"✓ Datasets created and cached successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model creation function\n",
                "def create_model(base_model_class, model_name, num_classes):\n",
                "    \"\"\"\n",
                "    Create a model with the specified base architecture.\n",
                "    \"\"\"\n",
                "    # Create base model\n",
                "    base_model = base_model_class(\n",
                "        weights='imagenet',\n",
                "        include_top=False,\n",
                "        input_shape=image_size + (3,)\n",
                "    )\n",
                "    \n",
                "    # Freeze base model initially\n",
                "    base_model.trainable = False\n",
                "    \n",
                "    # Build model\n",
                "    x = base_model.output\n",
                "    x = GlobalAveragePooling2D()(x)\n",
                "    \n",
                "    # Dense layers with regularization\n",
                "    x = Dense(512, activation='relu')(x)\n",
                "    x = BatchNormalization()(x)\n",
                "    x = Dropout(0.5)(x)\n",
                "    \n",
                "    x = Dense(256, activation='relu')(x)\n",
                "    x = BatchNormalization()(x)\n",
                "    x = Dropout(0.3)(x)\n",
                "    \n",
                "    x = Dense(128, activation='relu')(x)\n",
                "    x = Dropout(0.2)(x)\n",
                "    \n",
                "    # Output layer\n",
                "    predictions = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
                "    \n",
                "    model = Model(inputs=base_model.input, outputs=predictions)\n",
                "    \n",
                "    # Compile model\n",
                "    model.compile(\n",
                "        optimizer=Adam(learning_rate=0.001),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy'],\n",
                "        jit_compile=True\n",
                "    )\n",
                "    \n",
                "    return model, base_model\n",
                "\n",
                "# Fine-tuning function\n",
                "def fine_tune_model(model, base_model, num_layers_to_unfreeze=20):\n",
                "    \"\"\"\n",
                "    Fine-tune the model by unfreezing top layers.\n",
                "    \"\"\"\n",
                "    base_model.trainable = True\n",
                "    \n",
                "    # Freeze all layers except the last num_layers_to_unfreeze\n",
                "    for layer in base_model.layers[:-num_layers_to_unfreeze]:\n",
                "        layer.trainable = False\n",
                "    \n",
                "    # Recompile with lower learning rate\n",
                "    model.compile(\n",
                "        optimizer=Adam(learning_rate=0.0001),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Callbacks function\n",
                "def get_callbacks(model_name):\n",
                "    \"\"\"\n",
                "    Create callbacks for training.\n",
                "    \"\"\"\n",
                "    callbacks = [\n",
                "        EarlyStopping(\n",
                "            monitor='val_accuracy',\n",
                "            patience=5,\n",
                "            restore_best_weights=True,\n",
                "            verbose=1\n",
                "        ),\n",
                "        ReduceLROnPlateau(\n",
                "            monitor='val_loss',\n",
                "            factor=0.5,\n",
                "            patience=3,\n",
                "            min_lr=1e-7,\n",
                "            verbose=1\n",
                "        ),\n",
                "        ModelCheckpoint(\n",
                "            filepath=f'{model_name}_best.weights.h5',\n",
                "            monitor='val_accuracy',\n",
                "            save_best_only=True,\n",
                "            save_weights_only=True,\n",
                "            verbose=1\n",
                "        )\n",
                "    ]\n",
                "    return callbacks\n",
                "\n",
                "# Memory cleanup function\n",
                "def clear_memory():\n",
                "    \"\"\"\n",
                "    Clear memory between model trainings to prevent OOM errors.\n",
                "    \"\"\"\n",
                "    K.clear_session()\n",
                "    gc.collect()\n",
                "    print(\"✓ Memory cleared\")\n",
                "\n",
                "# Training function for a single model\n",
                "def train_single_model(model_name, base_model_class):\n",
                "    \"\"\"\n",
                "    Train a single model with two-stage training.\n",
                "    \"\"\"\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"TRAINING {model_name}\")\n",
                "    print(f\"{'='*80}\")\n",
                "    \n",
                "    # Create model\n",
                "    print(f\"\\nCreating {model_name} model...\")\n",
                "    model, base_model = create_model(base_model_class, model_name, num_classes)\n",
                "    print(f\"✓ {model_name} model created\")\n",
                "    \n",
                "    # Stage 1: Initial training\n",
                "    print(f\"\\n[STAGE 1] Initial training with frozen base model...\")\n",
                "    initial_epochs = 10\n",
                "    \n",
                "    history_stage1 = model.fit(\n",
                "        train_dataset,\n",
                "        validation_data=val_dataset,\n",
                "        epochs=initial_epochs,\n",
                "        callbacks=get_callbacks(f\"{model_name}_stage1\")\n",
                "    )\n",
                "    \n",
                "    # Stage 2: Fine-tuning\n",
                "    print(f\"\\n[STAGE 2] Fine-tuning with unfrozen top layers...\")\n",
                "    model = fine_tune_model(model, base_model, num_layers_to_unfreeze=20)\n",
                "    fine_tune_epochs = 10\n",
                "    \n",
                "    history_stage2 = model.fit(\n",
                "        train_dataset,\n",
                "        validation_data=val_dataset,\n",
                "        epochs=fine_tune_epochs,\n",
                "        callbacks=get_callbacks(f\"{model_name}_stage2\")\n",
                "    )\n",
                "    \n",
                "    # Save model\n",
                "    model.save(f\"{model_name}_plant_disease_model.h5\")\n",
                "    print(f\"\\n✓ {model_name} model saved\")\n",
                "    \n",
                "    # Generate predictions\n",
                "    print(f\"\\nGenerating predictions for {model_name}...\")\n",
                "    y_pred = model.predict(val_dataset)\n",
                "    \n",
                "    # Extract true labels\n",
                "    y_true = []\n",
                "    for image_batch, label_batch in val_dataset:\n",
                "        y_true.append(label_batch)\n",
                "    y_true = tf.concat(y_true, axis=0).numpy()\n",
                "    \n",
                "    # Save predictions\n",
                "    np.save(f\"{model_name}_y_pred.npy\", y_pred)\n",
                "    np.save(f\"{model_name}_y_true.npy\", y_true)\n",
                "    print(f\"✓ {model_name} predictions saved\")\n",
                "    \n",
                "    # Calculate accuracy\n",
                "    y_pred_labels = y_pred.argmax(axis=1)\n",
                "    y_true_labels = y_true.argmax(axis=1)\n",
                "    accuracy = np.mean(y_pred_labels == y_true_labels)\n",
                "    \n",
                "    print(f\"\\n{model_name} FINAL VALIDATION ACCURACY: {accuracy*100:.2f}%\")\n",
                "    \n",
                "    if accuracy >= 0.985:\n",
                "        print(f\"✓ {model_name} achieved >=98.5% accuracy!\")\n",
                "    else:\n",
                "        print(f\"✗ {model_name} did not reach 98.5% accuracy (got {accuracy*100:.2f}%)\")\n",
                "    \n",
                "    # Clear memory before next model\n",
                "    del model, base_model\n",
                "    clear_memory()\n",
                "    \n",
                "    return accuracy\n",
                "\n",
                "print(\"✓ Helper functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Model Training Cells\n",
                "**Run each cell below to train that specific model. Skip any cell to exclude that model from training.**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train VGG16\n",
                "try:\n",
                "    vgg16_accuracy = train_single_model('VGG16', VGG16)\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error training VGG16: {str(e)}\")\n",
                "    clear_memory()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train VGG19\n",
                "try:\n",
                "    vgg19_accuracy = train_single_model('VGG19', VGG19)\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error training VGG19: {str(e)}\")\n",
                "    clear_memory()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train InceptionV3\n",
                "try:\n",
                "    inceptionv3_accuracy = train_single_model('InceptionV3', InceptionV3)\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error training InceptionV3: {str(e)}\")\n",
                "    clear_memory()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Xception\n",
                "try:\n",
                "    xception_accuracy = train_single_model('Xception', Xception)\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error training Xception: {str(e)}\")\n",
                "    clear_memory()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train ResNet50\n",
                "try:\n",
                "    resnet50_accuracy = train_single_model('ResNet50', ResNet50)\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error training ResNet50: {str(e)}\")\n",
                "    clear_memory()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train DenseNet121\n",
                "try:\n",
                "    densenet121_accuracy = train_single_model('DenseNet121', DenseNet121)\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error training DenseNet121: {str(e)}\")\n",
                "    clear_memory()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Results & Comparison\n",
                "**Run the cells below after training your models to see the comparison.**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate comparison table\n",
                "from sklearn.metrics import precision_score, recall_score, f1_score\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"MODEL COMPARISON RESULTS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# List of all possible models\n",
                "all_models = ['VGG16', 'VGG19', 'InceptionV3', 'Xception', 'ResNet50', 'DenseNet121']\n",
                "results = []\n",
                "\n",
                "for model_name in all_models:\n",
                "    try:\n",
                "        # Load predictions\n",
                "        y_pred = np.load(f\"{model_name}_y_pred.npy\")\n",
                "        y_true = np.load(f\"{model_name}_y_true.npy\")\n",
                "        \n",
                "        # Convert to class labels\n",
                "        y_pred_labels = y_pred.argmax(axis=1)\n",
                "        y_true_labels = y_true.argmax(axis=1)\n",
                "        \n",
                "        # Calculate metrics\n",
                "        accuracy = np.mean(y_pred_labels == y_true_labels)\n",
                "        precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
                "        recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
                "        f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
                "        \n",
                "        # Top-5 accuracy\n",
                "        top5_pred = np.argsort(y_pred, axis=1)[:, -5:]\n",
                "        top5_accuracy = np.mean([y_true_labels[i] in top5_pred[i] for i in range(len(y_true_labels))])\n",
                "        \n",
                "        results.append({\n",
                "            'Model': model_name,\n",
                "            'Accuracy (%)': accuracy * 100,\n",
                "            'Precision (%)': precision * 100,\n",
                "            'Recall (%)': recall * 100,\n",
                "            'F1-Score (%)': f1 * 100,\n",
                "            'Top-5 Accuracy (%)': top5_accuracy * 100,\n",
                "            'Meets Target (>98.5%)': '✓' if accuracy > 0.985 else '✗'\n",
                "        })\n",
                "    except FileNotFoundError:\n",
                "        print(f\"⚠️  {model_name} was not trained (skipped)\")\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not load results for {model_name}: {str(e)}\")\n",
                "\n",
                "if len(results) > 0:\n",
                "    # Create DataFrame\n",
                "    comparison_df = pd.DataFrame(results)\n",
                "    comparison_df = comparison_df.sort_values('Accuracy (%)', ascending=False)\n",
                "    \n",
                "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    \n",
                "    # Count models above target\n",
                "    models_above_target = comparison_df[comparison_df['Meets Target (>98.5%)'] == '✓'].shape[0]\n",
                "    print(f\"\\nModels achieving >98.5% accuracy: {models_above_target}/{len(results)}\")\n",
                "    \n",
                "    # Save comparison table\n",
                "    comparison_df.to_csv('model_comparison_sequential.csv', index=False)\n",
                "    print(\"\\n✓ Comparison table saved to 'model_comparison_sequential.csv'\")\n",
                "else:\n",
                "    print(\"\\n⚠️  No models were trained. Please run at least one model training cell above.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize comparison\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "if len(results) > 0:\n",
                "    # Set style\n",
                "    sns.set_style('whitegrid')\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    \n",
                "    # Plot accuracy comparison\n",
                "    plt.subplot(1, 2, 1)\n",
                "    colors = ['green' if x == '✓' else 'red' for x in comparison_df['Meets Target (>98.5%)']]\n",
                "    plt.barh(comparison_df['Model'], comparison_df['Accuracy (%)'], color=colors, alpha=0.7)\n",
                "    plt.axvline(x=98.5, color='blue', linestyle='--', label='Target (98.5%)')\n",
                "    plt.xlabel('Accuracy (%)')\n",
                "    plt.title('Model Accuracy Comparison')\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    \n",
                "    # Plot metrics comparison\n",
                "    plt.subplot(1, 2, 2)\n",
                "    metrics_df = comparison_df[['Model', 'Precision (%)', 'Recall (%)', 'F1-Score (%)']].set_index('Model')\n",
                "    metrics_df.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
                "    plt.ylabel('Score (%)')\n",
                "    plt.title('Model Metrics Comparison')\n",
                "    plt.xticks(rotation=45, ha='right')\n",
                "    plt.legend(loc='lower right')\n",
                "    plt.tight_layout()\n",
                "    \n",
                "    plt.savefig('model_comparison_visualization.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"✓ Visualization saved to 'model_comparison_visualization.png'\")\n",
                "else:\n",
                "    print(\"⚠️  No results to visualize. Train at least one model first.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate detailed classification reports\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for model_name in all_models:\n",
                "    try:\n",
                "        y_pred = np.load(f\"{model_name}_y_pred.npy\")\n",
                "        y_true = np.load(f\"{model_name}_y_true.npy\")\n",
                "        \n",
                "        y_pred_labels = y_pred.argmax(axis=1)\n",
                "        y_true_labels = y_true.argmax(axis=1)\n",
                "        \n",
                "        print(f\"\\n{'='*80}\")\n",
                "        print(f\"{model_name} - Classification Report\")\n",
                "        print(f\"{'='*80}\")\n",
                "        \n",
                "        report = classification_report(y_true_labels, y_pred_labels, target_names=class_names)\n",
                "        print(report)\n",
                "        \n",
                "        # Save report\n",
                "        with open(f\"{model_name}_classification_report.txt\", 'w') as f:\n",
                "            f.write(f\"{model_name} - Classification Report\\n\")\n",
                "            f.write(\"=\"*80 + \"\\n\")\n",
                "            f.write(report)\n",
                "        \n",
                "        print(f\"✓ Report saved to '{model_name}_classification_report.txt'\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"\\n⚠️  {model_name} was not trained (skipped)\")\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not generate report for {model_name}: {str(e)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"FINAL SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "if len(results) > 0:\n",
                "    print(f\"\\nTotal models trained: {len(results)}\")\n",
                "    print(f\"Models achieving >98.5% accuracy: {models_above_target}\")\n",
                "    print(f\"\\nBest performing model: {comparison_df.iloc[0]['Model']}\")\n",
                "    print(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy (%)']:.2f}%\")\n",
                "else:\n",
                "    print(\"\\n⚠️  No models were trained.\")\n",
                "\n",
                "print(\"\\nKey features implemented:\")\n",
                "print(\"  ✓ Sequential training (one model at a time)\")\n",
                "print(\"  ✓ Separate cells for each model (skip any model)\")\n",
                "print(\"  ✓ Dataset caching for faster training\")\n",
                "print(\"  ✓ Memory management between models\")\n",
                "print(\"  ✓ GPU memory growth enabled\")\n",
                "print(\"  ✓ Mixed precision training\")\n",
                "print(\"  ✓ Two-stage training with fine-tuning\")\n",
                "print(\"  ✓ Enhanced data augmentation\")\n",
                "print(\"  ✓ Learning rate scheduling\")\n",
                "print(\"  ✓ Early stopping and model checkpointing\")\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"TRAINING COMPLETE!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true,
            "isInternetEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
